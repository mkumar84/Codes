---
title: "Intro to sparklyr"
---

*Note: this tutorial originally comes from the wonderful people at sparklyr.*

Install these packages if you don't have them already

```{r, eval=FALSE}
install.packages(c("nycflights13", "Lahman"))
```

# Connect to Spark

The following command is how to connect to Spark on the CAC machines. If you take this script to work or somewhere else, the `spark_hom` parameter may be different.

```{r}
library(sparklyr)
sc <- spark_connect(master = "yarn-client", spark_home = "/usr/hdp/current/spark-client/")
```

# Load some data into Spark

There are two ways to load data into Spark:

1. Load some data regularly, then use the `copy_to` command.
2. Use sparklyr's `spark_read_*` commands to read data that's on HDFS.

Since, for now, I haven't put any data on HDFS, we'll use the first method. The data we'll load is the classic, simple iris dataset (which comes with `dplyr`) and some flights and baseball data, just for fun.


```{r}
library(dplyr)
iris_tbl <- copy_to(sc, iris)
head(iris_tbl)
flights_tbl <- copy_to(sc, nycflights13::flights, "flights")
head(flights_tbl)
batting_tbl <- copy_to(sc, Lahman::Batting, "batting")
head(batting_tbl)
```

# Manipulate Data

Now, we can use the dplyr verbs (like `filter`) to manipulate Spark DataFrames! 

```{r}
iris_tbl %>% filter(petal_width < 0.3)
```

When this command is run, a lot of magic happens behind the scenes. Spark's implementation of `filter` gets translated into a bunch of MapReduce jobs, and the data is ported around to each job, the jobs are run, and the results are aggregated back.

Let's look at another table:

```{r}
# filter by departure delay
flights_tbl %>% filter(dep_delay == 2)
```

```{r}
# an example from the dplyr library
delay <- flights_tbl %>% 
  group_by(tailnum) %>%
    summarise(count = n(), dist = mean(distance), delay = mean(arr_delay)) %>%
      filter(count > 20, dist < 2000, !is.na(delay)) %>%
        collect()

        delay
        ```

# Plot some data


```{r}
library(ggplot2)
ggplot(delay, aes(dist, delay)) +
  geom_point(aes(size = count), alpha = 1/2) +
    geom_smooth() +
      scale_size_area(max_size = 2)
      ```

# Using SQL
```{r}
library(DBI)
iris_preview <- dbGetQuery(sc, "SELECT * FROM iris LIMIT 10")
iris_preview
```

# Building a Decision Tree

# TODO: split into training and testing


```{r}
tree = ml_decision_tree(iris_tbl, Species ~ .)
str(tree)
summary(tree)
predicted = predict(tree, iris_tbl)
predicted

# In Spark, a command like `iris$Species` won't work, unfortunately. 
actual = iris %>% select(Species) %>% collect %>% .[["Species"]]


table(predicted, actual)

# TODO: Other stats
```

# NOTE - due to security configuration, the spark web ui will not work!

```{r}
# now disconnect from spark
spark_disconnect(sc)
```
